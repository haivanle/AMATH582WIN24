{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following code to load and normalize the dataset for training and testing\n",
    "# It will downlad the dataset into data subfolder (change to your data folder name)\n",
    "train_dataset = torchvision.datasets.FashionMNIST('data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST('data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "# Use the following code to create a validation set of 10%\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(train_dataset)),\n",
    "    train_dataset.targets,\n",
    "    stratify=train_dataset.targets,\n",
    "    test_size=0.1,\n",
    ")\n",
    "\n",
    "# Generate training and validation subsets based on indices\n",
    "train_split = Subset(train_dataset, train_indices)\n",
    "val_split = Subset(train_dataset, val_indices)\n",
    "\n",
    "# set batches sizes\n",
    "train_batch_size = 512\n",
    "test_batch_size = 256\n",
    "\n",
    "# Define dataloader objects that help to iterate over batches and samples for\n",
    "# training, validation and testing\n",
    "train_batches = DataLoader(train_split, batch_size=train_batch_size, shuffle=True)\n",
    "val_batches = DataLoader(val_split, batch_size=train_batch_size, shuffle=True)\n",
    "test_batches = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustable parameters\n",
    "num_train_batches = len(train_batches)\n",
    "num_val_batches = len(val_batches)\n",
    "num_test_batches = len(test_batches)\n",
    "input_dim = 784  # Input dimension\n",
    "output_dim = 10  # Output dimension (number of classes)\n",
    "num_hidden_layers = 2  # Adjustable number of hidden layers\n",
    "hidden_dim = [400, 400]  # Adjustable number of neurons in each hidden layer\n",
    "learning_rate = 5e-2  # Adjustable learning rate\n",
    "num_epochs = 15  # Adjustable number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your (As Cool As It Gets) Fully Connected Neural Network \n",
    "class ACAIGFCN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden_layers, hidden_dim):\n",
    "        super(ACAIGFCN, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dim:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|          | 0/106 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 40.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Training Loss: 1.0389211749500697, Validation Accuracy: 77.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 46.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Training Loss: 0.6003602162643715, Validation Accuracy: 80.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 47.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Training Loss: 0.5114392004189667, Validation Accuracy: 81.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 46.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Training Loss: 0.4723648733386287, Validation Accuracy: 82.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 44.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Training Loss: 0.4391096002172541, Validation Accuracy: 82.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 37.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Training Loss: 0.4230526761478848, Validation Accuracy: 83.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 44.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Training Loss: 0.40402703648143345, Validation Accuracy: 84.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 45.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Training Loss: 0.38834971942725005, Validation Accuracy: 86.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 44.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Training Loss: 0.3770780355577116, Validation Accuracy: 85.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 44.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Training Loss: 0.3691605603430006, Validation Accuracy: 85.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 41.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Training Loss: 0.3621551866001553, Validation Accuracy: 86.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 47.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Training Loss: 0.35045969994862874, Validation Accuracy: 86.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 47.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Training Loss: 0.3452149110811728, Validation Accuracy: 87.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 47.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Training Loss: 0.33604657580234387, Validation Accuracy: 86.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 46.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Training Loss: 0.32954068407305964, Validation Accuracy: 87.27%\n",
      "Test Accuracy: 86.01%\n"
     ]
    }
   ],
   "source": [
    "# Initialize neural network model with input, output and hidden layer dimensions\n",
    "model = ACAIGFCN(input_dim, output_dim, num_hidden_layers, hidden_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Iterate over epochs, batches with progress bar and train+ validate the ACAIGFCN\n",
    "# Track the loss and validation accuracy\n",
    "# ACAIGFCN Training\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model into training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm.tqdm(train_batches, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.view(inputs.size(0), -1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_split)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # ACAIGFCN Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Telling PyTorch we aren't passing inputs to network for training purpose\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_batches:\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    # Record accuracy for the epoch; print training loss, validation accuracy\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_batches:\n",
    "        outputs = model(inputs.view(inputs.size(0), -1))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training loss curve\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss curve and validation accuracy curve\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training Loss and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('582hw4f1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, define a function based on the above model that allows us to quickly try out different optimizer\n",
    "def train_model(optimizer, learning_rate, num_epochs=15):\n",
    "    # Initialize model\n",
    "    model = ACAIGFCN(input_dim, output_dim, num_hidden_layers, hidden_dim)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm.tqdm(train_batches, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_split)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_batches:\n",
    "                outputs = model(inputs.view(inputs.size(0), -1))\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_batches:\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    return train_losses, val_accuracies, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    'SGD': optim.SGD,\n",
    "    'RMSProp': optim.RMSprop,\n",
    "    'Adam': optim.Adam\n",
    "}\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer and learning rate 0.001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 42.38it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 46.44it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 45.88it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 46.94it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 47.08it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 43.43it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 41.89it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 39.31it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 45.10it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 45.73it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 44.32it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 46.82it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 41.96it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 44.87it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 47.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer and learning rate 0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 47.45it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 44.79it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 46.85it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 46.35it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 46.95it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 47.20it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 47.21it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 46.72it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 47.45it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 47.62it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 42.94it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 47.48it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 46.23it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 47.44it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 47.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer and learning rate 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 47.10it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 47.08it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 47.20it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 47.13it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 46.80it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 45.67it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 46.40it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 47.33it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 47.85it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 46.13it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 46.92it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 45.18it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 46.50it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 44.50it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 42.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RMSProp optimizer and learning rate 0.001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 46.25it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 44.29it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 46.42it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 46.27it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 46.26it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 41.92it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 44.30it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 45.95it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 46.17it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 43.62it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 44.55it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 45.63it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 46.40it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 45.64it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 45.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RMSProp optimizer and learning rate 0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 45.82it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 46.20it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 46.07it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 42.52it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 40.80it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 37.60it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 41.54it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 45.40it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 46.45it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 45.78it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 45.53it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 42.69it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 43.63it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 42.20it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 45.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RMSProp optimizer and learning rate 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 45.37it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 45.08it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 44.81it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 44.92it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 43.98it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 45.21it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 42.28it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 44.74it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 41.29it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 43.64it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 44.21it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 45.12it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 45.04it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 45.45it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 45.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer and learning rate 0.001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 44.75it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 44.81it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 44.54it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 42.21it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 42.85it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 44.39it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 44.60it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 43.97it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 44.31it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 44.55it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 44.86it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 44.67it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 43.37it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 40.09it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 41.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer and learning rate 0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 40.78it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 41.08it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 44.74it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 44.03it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 41.53it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 40.50it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 42.10it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 40.73it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 39.13it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 44.13it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 43.75it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 44.30it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 43.90it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 43.94it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 43.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer and learning rate 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 42.70it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 42.98it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 41.06it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 42.55it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 42.13it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 41.78it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 37.81it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 39.69it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 41.01it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 40.25it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 40.49it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 38.27it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 40.40it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 39.93it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 39.85it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for optimizer_name, optimizer_func in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        print(f'Training with {optimizer_name} optimizer and learning rate {lr}...')\n",
    "        train_losses, val_accuracies, test_accuracy = train_model(optimizer_func, lr)\n",
    "        results[(optimizer_name, lr)] = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'test_accuracy': test_accuracy\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare the performance of different optimizers\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (optimizer_name, lr), result in results.items():\n",
    "    plt.plot(result['train_losses'], label=f'{optimizer_name} (lr={lr:.3f})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Curves for Different Optimizers and Learning Rates')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (optimizer_name, lr), result in results.items():\n",
    "    plt.plot(result['val_accuracies'], label=f'{optimizer_name} (lr={lr:.3f})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy Curves for Different Optimizers and Learning Rates')\n",
    "plt.legend()\n",
    "plt.savefig('582hw4f2.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Print test accuracies\n",
    "print('\\nTest Accuracies:')\n",
    "for (optimizer_name, lr), result in results.items():\n",
    "    print(f'{optimizer_name} (lr={lr:.3f}): {result[\"test_accuracy\"] * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new FCN that takes dropout probability as one of the parameters\n",
    "class ACAIGFCNWithDropout(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden_layers, hidden_dim, dropout_prob=0.5):\n",
    "        super(ACAIGFCNWithDropout, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dim:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_dropout(optimizer, learning_rate, num_epochs=15, dropout_prob=0.5):\n",
    "    # Initialize model with dropout regularization\n",
    "    model = ACAIGFCNWithDropout(input_dim, output_dim, num_hidden_layers, hidden_dim, dropout_prob)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm.tqdm(train_batches, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_split)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_batches:\n",
    "                outputs = model(inputs.view(inputs.size(0), -1))\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_batches:\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    return train_losses, val_accuracies, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer and learning rate 0.001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 39.99it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 43.56it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 44.65it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 45.09it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 44.26it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 43.18it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 44.18it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 43.10it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 42.31it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 41.30it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 42.30it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 44.30it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 40.91it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 41.40it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 42.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer and learning rate 0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 42.62it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 43.88it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 43.18it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 43.87it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 41.18it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 40.95it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 39.57it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 40.47it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 39.68it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 40.53it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 41.50it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 38.88it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 37.41it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 38.43it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 41.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer and learning rate 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 44.01it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 43.93it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 44.89it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 42.47it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 41.23it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 42.17it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 41.83it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 43.64it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 40.93it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 43.37it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 42.38it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 43.91it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 41.42it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 44.39it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 44.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RMSProp optimizer and learning rate 0.001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 43.87it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 41.53it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 42.40it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 43.80it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 43.54it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 44.09it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 43.61it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 43.89it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 42.65it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 43.86it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 43.57it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 43.92it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 43.42it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 44.08it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 43.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RMSProp optimizer and learning rate 0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 42.70it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 43.78it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 39.04it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 43.29it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 42.41it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 39.20it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 39.03it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 41.90it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 43.54it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 43.95it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 41.98it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 40.99it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 39.12it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 42.41it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 38.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RMSProp optimizer and learning rate 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 41.74it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 42.88it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 43.11it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 43.42it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 43.70it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 44.34it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 40.83it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 44.02it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 43.62it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 42.87it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 42.97it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 41.11it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 42.42it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 40.31it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 41.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer and learning rate 0.001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 41.60it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 40.84it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 40.80it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 42.17it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 41.78it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:03<00:00, 33.98it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 42.15it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 41.00it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 40.60it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 42.16it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 39.42it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 39.76it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 38.90it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 42.19it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer and learning rate 0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 39.70it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 41.15it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 37.64it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:03<00:00, 34.87it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 38.32it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 40.05it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 39.03it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 38.31it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 38.47it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 39.42it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 37.17it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 40.18it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 39.87it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 38.64it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 40.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer and learning rate 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 38.48it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 39.30it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 35.85it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 37.62it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 40.48it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 40.54it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 39.99it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 39.04it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 36.57it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 40.09it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 39.26it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 42.10it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 38.76it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 39.07it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 39.66it/s]\n"
     ]
    }
   ],
   "source": [
    "dropout_results = {}\n",
    "\n",
    "for optimizer_name, optimizer_func in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        print(f'Training with {optimizer_name} optimizer and learning rate {lr}...')\n",
    "        dropout_train_losses, dropout_val_accuracies, dropout_test_accuracy = train_model_with_dropout(optimizer_func, lr)\n",
    "        dropout_results[(optimizer_name, lr)] = {\n",
    "            'train_losses': dropout_train_losses,\n",
    "            'val_accuracies': dropout_val_accuracies,\n",
    "            'test_accuracy': dropout_test_accuracy\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (optimizer_name, lr), result in dropout_results.items():\n",
    "    plt.plot(result['train_losses'], label=f'{optimizer_name} (lr={lr:.3f})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Curves with Dropout Regularization')\n",
    "plt.legend()\n",
    "plt.savefig('582hw4f4.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for (optimizer_name, lr), result in dropout_results.items():\n",
    "    plt.plot(result['val_accuracies'], label=f'{optimizer_name} (lr={lr:.3f})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy Curves with Dropout Regularization')\n",
    "plt.legend()\n",
    "plt.savefig('582hw4f3.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Print test accuracies\n",
    "print('\\nTest Accuracies with Dropout Regularization:')\n",
    "for (optimizer_name, lr), result in dropout_results.items():\n",
    "    print(f'{optimizer_name} (lr={lr:.3f}): {result[\"test_accuracy\"] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new FCN to test different initializations\n",
    "class ACAIGFCNWithInitialization(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden_layers, hidden_dim, initialization):\n",
    "        super(ACAIGFCNWithInitialization, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dim:\n",
    "            layer = nn.Linear(prev_dim, dim)\n",
    "            initialization(layer.weight.data)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_initialization(optimizer, learning_rate, initialization, num_epochs=15):\n",
    "    # Initialize model with specified initialization\n",
    "    model = ACAIGFCNWithInitialization(input_dim, output_dim, num_hidden_layers, hidden_dim, initialization)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm.tqdm(train_batches, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_split)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_batches:\n",
    "                outputs = model(inputs.view(inputs.size(0), -1))\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_batches:\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    return train_losses, val_accuracies, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer, learning rate 0.001, and initialization Random Normal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 42.40it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 43.89it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 44.64it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 45.24it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 42.22it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 44.03it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 41.81it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 44.57it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 44.17it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 44.12it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 43.77it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 43.77it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 44.91it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 46.04it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 45.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer, learning rate 0.001, and initialization Xavier Normal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 40.39it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 42.22it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 44.73it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 44.58it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 46.05it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 45.75it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 43.82it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 44.32it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 43.02it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 42.20it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 45.22it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 45.01it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 45.69it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 45.74it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 45.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer, learning rate 0.001, and initialization Kaiming Uniform...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 45.22it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 44.00it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 39.45it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 37.05it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 40.47it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 42.16it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 40.50it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 41.65it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 42.73it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 42.16it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 40.84it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 44.40it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 42.59it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 43.07it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 44.74it/s]\n"
     ]
    }
   ],
   "source": [
    "initializations = {\n",
    "    'Random Normal': nn.init.normal_,\n",
    "    'Xavier Normal': nn.init.xavier_normal_,\n",
    "    'Kaiming Uniform': nn.init.kaiming_uniform_\n",
    "}\n",
    "\n",
    "results_initialization = {}\n",
    "# Given my laptop capacity, I modified the baseline configuration and picked Adam optimizer with learning rate 0.001 to test the initializations\n",
    "# as they are the best performer\n",
    "for initialization_name, initialization_func in initializations.items():\n",
    "    print(f'Training with Adam optimizer, learning rate 0.001, and initialization {initialization_name}...')\n",
    "    train_losses, val_accuracies, test_accuracy = train_model_with_initialization(optim.Adam, 0.001, initialization_func)\n",
    "    results_initialization[(optim.Adam, 0.001, initialization_name)] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'test_accuracy': test_accuracy\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Random Normal Initialization: 88.96%\n",
      "Test Accuracy with Xavier Normal Initialization: 88.96%\n",
      "Test Accuracy with Kaiming Uniform Initialization: 88.96%\n"
     ]
    }
   ],
   "source": [
    "for initialization_name, initialization_func in initializations.items():\n",
    "    print(f'Test Accuracy with {initialization_name} Initialization: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a new FCN to test batch normalization, not considering initializations\n",
    "# class ACAIGFCNWithNormalization(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, num_hidden_layers, hidden_dim):\n",
    "#         super(ACAIGFCNWithNormalization, self).__init__()\n",
    "#         layers = []\n",
    "#         prev_dim = input_dim\n",
    "#         for dim in hidden_dim:\n",
    "#             layers.append(nn.Linear(prev_dim, dim))\n",
    "#             layers.append(nn.BatchNorm1d(dim))  # Batch Normalization\n",
    "#             layers.append(nn.ReLU())\n",
    "#             prev_dim = dim\n",
    "#         layers.append(nn.Linear(prev_dim, output_dim))\n",
    "#         self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Given my laptop capacity, I modified the baseline configuration and picked Adam optimizer with learning rate 0.001 as they are the best performer\n",
    "# def train_model_with_normalization(num_epochs=15):\n",
    "#     # Initialize model with Batch Normalization\n",
    "#     model = ACAIGFCNWithNormalization(input_dim, output_dim, num_hidden_layers, hidden_dim)\n",
    "\n",
    "#     # Loss and optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#     # Training loop\n",
    "#     train_losses = []\n",
    "#     val_accuracies = []\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, labels in train_batches:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs.view(inputs.size(0), -1))\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#         epoch_loss = running_loss / len(train_split)\n",
    "#         train_losses.append(epoch_loss)\n",
    "\n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_batches:\n",
    "#                 outputs = model(inputs.view(inputs.size(0), -1))\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "#         val_accuracy = correct / total\n",
    "#         val_accuracies.append(val_accuracy)\n",
    "\n",
    "#     # Testing\n",
    "#     model.eval()\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_batches:\n",
    "#             outputs = model(inputs.view(inputs.size(0), -1))\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "#     test_accuracy = test_correct / test_total\n",
    "\n",
    "#     return train_losses, val_accuracies, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Batch Normalization: 88.84%\n"
     ]
    }
   ],
   "source": [
    "# # Train model with Batch Normalization\n",
    "# train_losses_with_normalization, val_accuracies_with_normalization, test_accuracy_with_normalization = train_model_with_normalization()\n",
    "\n",
    "# print(f'Test Accuracy with Batch Normalization: {test_accuracy_with_normalization * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACAIGFCNWithNormalization(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden_layers, hidden_dim, initialization):\n",
    "        super(ACAIGFCNWithNormalization, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dim:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.BatchNorm1d(dim))  # Batch Normalization\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        # Initialization\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                initialization(m.weight)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_normalization(optimizer, learning_rate, initialization, num_epochs=15):\n",
    "    # Initialize model with specified initialization\n",
    "    model = ACAIGFCNWithNormalization(input_dim, output_dim, num_hidden_layers, hidden_dim, initialization)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm.tqdm(train_batches, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_split)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_batches:\n",
    "                outputs = model(inputs.view(inputs.size(0), -1))\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_batches:\n",
    "            outputs = model(inputs.view(inputs.size(0), -1))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = test_correct / test_total\n",
    "\n",
    "    return train_losses, val_accuracies, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam optimizer, learning rate 0.001, initialization Random Normal, and Batch Normalization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 40.38it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 45.99it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 46.28it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 43.45it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 41.10it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 42.16it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 43.94it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 43.77it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 41.84it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 43.47it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 41.37it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 42.50it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 39.90it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 44.64it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 43.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Random Normal Initialization: 84.78%\n",
      "Training with Adam optimizer, learning rate 0.001, initialization Xavier Normal, and Batch Normalization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 44.86it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 45.39it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 41.99it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 42.17it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 40.27it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 38.34it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 44.50it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 41.55it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:02<00:00, 42.54it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:02<00:00, 45.09it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 45.39it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 43.38it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 45.11it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 44.09it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 45.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Xavier Normal Initialization: 88.75%\n",
      "Training with Adam optimizer, learning rate 0.001, initialization Kaiming Uniform, and Batch Normalization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 106/106 [00:02<00:00, 36.78it/s]\n",
      "Epoch 2/15: 100%|██████████| 106/106 [00:02<00:00, 37.59it/s]\n",
      "Epoch 3/15: 100%|██████████| 106/106 [00:02<00:00, 42.80it/s]\n",
      "Epoch 4/15: 100%|██████████| 106/106 [00:02<00:00, 41.03it/s]\n",
      "Epoch 5/15: 100%|██████████| 106/106 [00:02<00:00, 42.34it/s]\n",
      "Epoch 6/15: 100%|██████████| 106/106 [00:02<00:00, 43.61it/s]\n",
      "Epoch 7/15: 100%|██████████| 106/106 [00:02<00:00, 42.05it/s]\n",
      "Epoch 8/15: 100%|██████████| 106/106 [00:02<00:00, 44.71it/s]\n",
      "Epoch 9/15: 100%|██████████| 106/106 [00:03<00:00, 32.74it/s]\n",
      "Epoch 10/15: 100%|██████████| 106/106 [00:03<00:00, 32.11it/s]\n",
      "Epoch 11/15: 100%|██████████| 106/106 [00:02<00:00, 41.10it/s]\n",
      "Epoch 12/15: 100%|██████████| 106/106 [00:02<00:00, 44.27it/s]\n",
      "Epoch 13/15: 100%|██████████| 106/106 [00:02<00:00, 43.51it/s]\n",
      "Epoch 14/15: 100%|██████████| 106/106 [00:02<00:00, 42.86it/s]\n",
      "Epoch 15/15: 100%|██████████| 106/106 [00:02<00:00, 43.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Kaiming Uniform Initialization: 88.50%\n"
     ]
    }
   ],
   "source": [
    "initializations = {\n",
    "    'Random Normal': nn.init.normal_,\n",
    "    'Xavier Normal': nn.init.xavier_normal_,\n",
    "    'Kaiming Uniform': nn.init.kaiming_uniform_\n",
    "}\n",
    "results_initialization = {}\n",
    "\n",
    "for initialization_name, initialization_func in initializations.items():\n",
    "    print(f'Training with Adam optimizer, learning rate 0.001, initialization {initialization_name}, and Batch Normalization...')\n",
    "    train_losses, val_accuracies, test_accuracy = train_model_with_normalization(optim.Adam, 0.001, initialization_func)\n",
    "    results_initialization[(initialization_name)] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'test_accuracy': test_accuracy\n",
    "    }\n",
    "    print(f'Test Accuracy with {initialization_name} Initialization: {test_accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EM6GQLv6j5uH"
   ],
   "name": "Lab 2- PyTorch Basics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
